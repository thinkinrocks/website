[
  {
    "id": "varjo-xr3",
    "name": "Varjo XR-3",
    "description": "The Varjo XR-3 is a professional mixed reality headset offering human-eye resolution (over 70 PPD), dual 12 MP pass-through cameras, LiDAR depth sensing, 200 Hz eye tracking, Ultraleap hand tracking, and a 115° field of view.",
    "cloudinaryPublicId": "varjo-xr3_t353nm",
    "categories": [
      "XR/VR",
      "Computer Vision"
    ],
    "status": "available",
    "availableSince": "2025-09-08",
    "details": [
      "The [Varjo XR-3](https://varjo.com/products/varjo-xr-3/) is designed for enterprise and research applications where accuracy and immersion are paramount. It delivers over 70 PPD in the focus area, with a wide 115° FOV and photorealistic dual 12 MP pass-through combined with LiDAR depth sensing.",
      "It features 200 Hz eye tracking with sub-degree accuracy and integrated Ultraleap Geminihand tracking, making it suitable for precision-intensive workflows like industrial training, medical simulation, and architectural visualization.",
      "System setup requires a powerful workstation that meets [Varjo’s requirements](https://varjo.com/use-center/system-requirements/). You must install Varjo Base and follow the official [headset setup guide](https://support.varjo.com/hc/en-us/setting-up-varjo-headset). Calibration is critical to ensure optimal image quality and tracking performance.",
      "Development typically uses Unity XR or Unreal Engine XR frameworks, together with Varjo SDKs and plugins. Start with sample projects, then optimize performance using foveated rendering and efficient rendering pipelines.",
      "Applications include AR-assisted manufacturing workflows, digital twin control rooms, and gaze-driven analytics in medical training. Researchers use its precise tracking for human–computer interaction and neuroscience studies.",
      "The XR-3 is not a consumer device: it requires external tracking (SteamVR 2.0 recommended), a high-end GPU workstation, and professional setup. It is optimized for dedicated labs and enterprise environments rather than portable use.",
      "**Essential Resources:**<br/>- [Varjo Developer Portal](https://developer.varjo.com/)<br/>- [Varjo Base Software](https://varjo.com/products/varjo-base/)<br/>- [Unity XR Docs](https://docs.unity3d.com/6000.2/Documentation/Manual/XR.html)<br/>- [Unreal AR/VR Guide](https://dev.epicgames.com/documentation/en-us/unreal-engine/developing-for-xr-experiences-in-unreal-engine?application_version=5.0)"
    ],
    "related": [
      "shelby-computer",
      "lenovo-legion-t5",
      "doublepoint-dev-kit"
    ]
  },
  {
    "id": "doublepoint-dev-kit",
    "name": "Doublepoint Dev Kit",
    "description": "The Doublepoint Developer Kit is a wearable sensor that enables precise wrist and finger gesture recognition. It supports low-latency, natural input for XR, wearables, and mobile devices, making interactions more intuitive.",
    "cloudinaryPublicId": "doublepoint_imsl0w",
    "categories": [
      "XR/VR"
    ],
    "status": "available",
    "availableSince": "2025-09-24",
    "details": [
      "The [Doublepoint Developer Kit](https://www.doublepoint.com/) provides advanced gesture recognition through a compact wrist-worn device. It captures subtle finger and wrist movements—such as taps, pinches, flicks, and rotations—with low latency, enabling natural, controller-free input for XR headsets, wearables, and mobile applications.",
      "Developers can access SDKs and sample apps via the [Doublepoint Github page](https://github.com/doublepointlab). Setup involves pairing the device, running calibration, and integrating gesture events into Unity, Unreal Engine, or mobile frameworks. Calibration ensures accuracy, but also means per-user setup is recommended for optimal performance.",
      "The device is particularly suited for XR interfaces, accessibility applications, and hands-busy environments like healthcare or manufacturing. It also enables projects like gesture-based smart home remotes, or experimental sign-language interpretation systems.",
      "Designing intuitive gesture-to-action mappings is key: start with simple, reliable gestures and expand gradually. Provide clear feedback and tune thresholds to reduce false positives. Since gestures vary between users, testing across diverse participants is essential.",
      "Limitations include the need for calibration, limited gesture vocabulary compared to full optical hand tracking, and battery life constraints—best suited for intermittent rather than continuous use.",
      "**Key Resources:**<br/>- [Doublepoint Github](https://github.com/doublepointlab)<br/>- [Unity Input System](https://docs.unity3d.com/Packages/com.unity.inputsystem@latest)<br/>- [Apple Accessibility Guidelines](https://developer.apple.com/accessibility/)"
    ],
    "related": [
      "varjo-xr3",
      "emotibit-bundle",
      "openbci-biosensing-bundle"
    ]
  },
  {
    "id": "alifensemble",
    "name": "Alif Ensemble DevKit",
    "description": "The Alif Ensemble DevKit Gen 2 is a powerful development board built on the Ensemble E7 fusion processor. It enables rapid prototyping for embedded AI, real-time control, and ultra-low-power edge computing applications.",
    "cloudinaryPublicId": "alif_semi_flnyws",
    "categories": [
      "AI",
      "Embedded Systems"
    ],
    "status": "available",
    "availableSince": "2025-10-02",
    "details": [
      "The [Alif Ensemble E7 DevKit Gen 2 (DK-E7)](https://alifsemi.com/support/kits/ensemble-e7devkit) is a flexible single-board platform supporting configuration as E1, E3, E5, or E7-class devices. It showcases Alif’s Ensemble fusion processor family, designed for intelligent edge applications that combine AI acceleration, real-time control, and ultra-low-power operation.",
      "The SoC integrates dual Cortex-A32 application cores, dual Cortex-M55 real-time cores (HE and HP variants), and dual Ethos-U55 microNPUs, offering up to 480 MHz compute frequency for mixed workloads. This heterogeneous architecture enables efficient distribution of tasks between AI inference and deterministic control. [User Guide (PDF)](https://alifsemi.com/download/AUGD0010)",
      "Connectivity and peripherals include Ethernet, dual USB (host/device), SDMMC/SDIO, MIPI-DSI display, MIPI-CSI camera, UARTs, SPI, I²C, ADC, PWM, and extensive GPIO headers. The board also features on-board power regulation, debug interfaces (JTAG/SWD), and a camera expansion port for vision applications. [Mouser Product Page](https://www.mouser.com/new/alif-semiconductor/alif-semi-dk-e7-development-kit/)",
      "Development is supported via the [Alif Ensemble SDK](https://github.com/alifsemi/ensemble_SDK), which includes BSPs, sample apps, and support for FreeRTOS, Azure RTOS, and Zephyr. Toolchains include GCC and Arm Clang, with integration examples for Edge Impulse ML workflows.",
      "To get started, connect power via micro-USB, configure jumpers for power and boot options, and install the SETools for secure firmware handling. Applications can be flashed or debugged using standard UART or JTAG interfaces. The SDK provides templates for sensor data collection, gesture recognition, and keyword spotting AI demos.",
      "The DevKit is officially supported by [Edge Impulse](https://www.edgeimpulse.com/blog/announcing-official-support-for-the-alif-ensemble-e7-development-kit), enabling data capture, model training, and deployment directly onto the Ensemble E7 cores. This makes it ideal for always-on AI applications like predictive maintenance, environmental sensing, and gesture or speech recognition at the edge.",
      "Advantages include efficient multi-core AI processing, strong peripheral support, and scalable architecture from low-power microcontrollers to application-class processors. Limitations include the need for manual configuration for multi-core scheduling, a moderate learning curve for new SDK users, and unsuitability for high-end vision or large-model inference workloads.",
      "**Key Resources:**<br/> - [Alif Ensemble SDK (GitHub)](https://github.com/alifsemi/ensemble_SDK)<br/> - [Ensemble E7 User Guide (PDF)](https://alifsemi.com/download/AUGD0010)<br/> - [Edge Impulse Integration Guide](https://www.edgeimpulse.com/blog/announcing-official-support-for-the-alif-ensemble-e7-development-kit)"
    ],
    "related": [
      "jetson-orin-nano",
      "esp-dev-boards",
      "raspberry-pi-5"
    ]
  },
  {
    "id": "shelby-computer",
    "name": "Shelby Computer",
    "description": "Custom AI workstation with Tenstorrent Blackhole™ P100a accelerator, AMD Ryzen 9 9950X3D CPU, 64GB DDR5 RAM, and 2TB NVMe SSD. Optimized for large-scale model training, reinforcement learning, and prototyping.",
    "cloudinaryPublicId": "shelby-computer_cndb2j",
    "categories": [
      "Workstation",
      "AI"
    ],
    "status": "available",
    "availableSince": "2025-09-18",
    "details": [
      "Shelby is a custom-built AI workstation designed for computationally demanding research and prototyping. At its core is an AMD Ryzen 9 9950X3D CPU paired with the [Tenstorrent Blackhole™ P100a](https://tenstorrent.com/) accelerator, supported by 64GB of high-speed DDR5 RAM and a Samsung 990 PRO 2TB NVMe SSD for fast data access. The system is housed in a Blackstorm Artemis A711G ATX case with liquid cooling and a 1200W PSU, ensuring stable performance during sustained workloads.",
      "With this setup, Shelby can train and fine-tune large AI models such as LLMs, vision transformers, and diffusion architectures. It is well-suited for reinforcement learning experiments, real-time vision inference, robotics control, and compiler research for emerging RISC-V accelerators. Compared to conventional GPU workstations, it offers a unique blend of CPU power and accelerator efficiency for both mainstream and experimental AI workflows.",
      "To get started, install a Linux distribution (Ubuntu is recommended) and configure the Tenstorrent software stack. After setting up the drivers, you can use standard ML frameworks such as [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/) together with the Tenstorrent SDK. Containerization with [Docker](https://docs.docker.com/) or environment managers like [Conda](https://docs.conda.io/) is strongly encouraged to keep experiments reproducible. Begin with smaller models to validate the environment before scaling up.",
      "Shelby enables projects ranging from domain-specific LLM training and multimodal AI exploration to robotics simulations, real-time computer vision, and experimental AI compiler development. Teams can use it as a shared compute backbone for prototyping ideas that are otherwise too resource-intensive for laptops or cloud credits.",
      "There are important limitations to keep in mind. The system has a high power draw and heat output, requiring proper ventilation and cooling. The Tenstorrent ecosystem is less mature than CUDA, so expect a steeper learning curve, additional debugging, and performance tuning. Advanced projects may require compiler-level optimization for full efficiency.",
      "Before using Shelby, users should be comfortable with Linux, Docker or Conda environments, and at least one deep learning framework (PyTorch or TensorFlow). Familiarity with containerized workflows and debugging hardware-accelerated ML environments will make onboarding smoother and reduce time spent on setup issues."
    ],
    "related": [
      "tenstorrent-blackhole",
      "jetson-orin-nano",
      "lenovo-legion-t5"
    ]
  },
  {
    "id": "nexys-a7-fpga",
    "name": "Nexys A7 FPGA Trainer Board",
    "description": "The Nexys A7 is an FPGA trainer board built on the AMD Artix-7 architecture. It features abundant I/O, DDR2 memory, and built-in peripherals, making it ideal for digital design, embedded systems, and hardware prototyping.",
    "cloudinaryPublicId": "nexys-a7_g2jiyh",
    "categories": [
      "FPGA"
    ],
    "status": "available",
    "availableSince": "2025-09-17",
    "details": [
      "The [Nexys A7](https://digilent.com/shop/nexys-a7-fpga-trainer-board-recommended-for-ece-curriculum/) is built around the AMD Artix-7 XC7A100T or XC7A50T FPGA. It provides a rich set of peripherals including LEDs, switches, buttons, VGA output, audio codec, USB-UART, Ethernet, and DDR2 memory, making it an ideal entry point for FPGA education and hardware prototyping.",
      "The board supports projects ranging from basic digital logic to full CPU architectures, DSP pipelines, and custom peripheral interfaces. It is widely used in ECE curricula for teaching FPGA design, embedded systems, and computer architecture.",
      "Development begins with [Vivado Design Suite](https://www.amd.com/en/products/software/adaptive-socs-and-fpgas/vivado.html), AMD’s FPGA toolchain. Create a new project targeting the Nexys A7 to automatically configure the correct device and constraint files. Start with simple designs in Verilog or VHDL, simulate them, then synthesize and implement for deployment to the board via JTAG.",
      "A typical workflow includes: HDL design entry, functional simulation, synthesis, place & route, and hardware programming. Begin with small designs (combinational logic, counters) before scaling to sequential systems, processors, or real-time signal processing.",
      "Project ideas include implementing a RISC-V soft-core CPU with custom I/O, developing audio processing pipelines with the onboard codec, VGA/HDMI video controllers, or LED matrix drivers. Advanced users can experiment with FPGA-based accelerators for real-time machine learning inference.",
      "While excellent for education, the Nexys A7 has resource limitations compared to larger FPGAs, which may constrain high-complexity designs. Timing closure can be challenging and requires good understanding of FPGA architecture and optimization techniques.",
      "To succeed, learners need a foundation in digital logic, proficiency in Verilog or VHDL, and skills in simulation and verification. Testbenches, timing analysis, and constraint management are essential as projects scale.",
      "**Key Resources:**<br/>- [Digilent Nexys A7 Reference](https://digilent.com/reference/programmable-logic/nexys-a7/start)<br/>- [Vivado Documentation](https://docs.xilinx.com/)<br/>- [FPGA4Student](https://www.fpga4student.com/)"
    ],
    "related": [
      "amd-kria-kr260",
      "amd-kria-kv260"
    ]
  },
  {
    "id": "tenstorrent-blackhole",
    "name": "Tenstorrent Blackhole™ p100a",
    "description": "The Tenstorrent Blackhole™ p100a is a PCIe accelerator card with 16 RISC-V cores and 28 GB of GDDR6 memory. Designed for AI training and inference, it operates at up to 300W in an active-cooled desktop form factor and is supported by Tenstorrent's open-source software stack.",
    "cloudinaryPublicId": "tenstorrent_igxdgh",
    "categories": [
      "AI"
    ],
    "status": "available",
    "availableSince": "2025-09-01",
    "details": [
      "The [Tenstorrent Blackhole™ p100a](https://tenstorrent.com/) introduces a new approach to AI acceleration, built on a RISC-V architecture with 16 high-performance cores and 28 GB of GDDR6 memory. Packaged as a PCIe board with active cooling, it delivers up to 300W of compute performance for workstations and research environments.",
      "It is designed for both training and inference of neural networks, offering an alternative to traditional GPU-based solutions. The architecture is particularly effective for large memory-intensive models such as LLMs, vision transformers, and reinforcement learning pipelines, while also enabling research into non-GPU compute paradigms.",
      "Setup requires Linux, a robust power supply, and adequate cooling capacity. Begin by installing the Tenstorrent software stack and drivers. Integration with frameworks like [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) is supported through bridge interfaces, allowing developers to port existing models with minimal changes.",
      "Development should start with vendor-provided benchmarks and examples to validate performance. Profiling workloads is essential to identify optimization opportunities, with iterative tuning often required to maximize throughput. The open-source nature of the stack also allows developers to customize compilers and runtime behavior.",
      "Use cases include efficient inference of large language models, quantization experiments, compiler and RISC-V accelerator research, and hybrid CPU/GPU/accelerator computing architectures. Researchers exploring next-generation ML hardware stacks benefit from its openness and programmability.",
      "The card’s ~300W power draw demands careful infrastructure planning. Adequate airflow, PSU capacity, and chassis compatibility are prerequisites. Compared to CUDA-based ecosystems, the Tenstorrent developer community is smaller, requiring developers to be comfortable with hands-on integration and contributing to open-source efforts.",
      "Success requires strong Linux skills, familiarity with ML frameworks, and experience in performance optimization. Knowledge of profiling, kernel tuning, and accelerator programming models will help unlock the hardware’s full potential.",
      "**Key Resources:**<br/>- [Tenstorrent Official Site](https://tenstorrent.com/)<br/>- [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/)<br/>- Tenstorrent Community (forums, GitHub) for open-source tools and developer discussions"
    ],
    "related": [
      "shelby-computer"
    ]
  },
  {
    "id": "jetson-orin-nano",
    "name": "NVIDIA Jetson Orin Nano Super",
    "description": "Compact edge AI module delivering 67 TOPS with Ampere GPU (1024 CUDA, 32 Tensor cores), 6-core Arm A78AE CPU, 8GB LPDDR5 (102 GB/s), and 7–25W configurable power. Optimized for running LLMs, vision models, and robotics workloads directly at the edge.",
    "cloudinaryPublicId": "jetson-nano_dypbw9",
    "categories": [
      "AI",
      "Computer Vision",
      "Robotics"
    ],
    "status": "available",
    "availableSince": "2025-09-01",
    "details": [
      "The [Jetson Orin Nano](https://developer.nvidia.com/embedded/jetson-orin) is NVIDIA’s smallest and most efficient Orin-based module. It provides up to 67 TOPS of AI compute via an Ampere GPU with 1024 CUDA cores and 32 Tensor cores, a 6-core Arm Cortex-A78AE CPU, and 8GB of LPDDR5 memory with 102 GB/s bandwidth. Its 7–25W configurable TDP enables deployment in compact, power-sensitive robotics and edge AI systems.",
      "The module runs real-time inference for computer vision, natural language, and transformer-based models through TensorRT acceleration. Applications include autonomous robots, drones, and smart vision systems that require on-device intelligence with low latency and no dependency on cloud services.",
      "Development begins with flashing JetPack OS using NVIDIA’s [SDK Manager](https://developer.nvidia.com/nvidia-sdk-manager). JetPack provides CUDA, cuDNN, TensorRT, and other optimized libraries for ML/DL workloads. Once installed, you can run PyTorch, TensorFlow, and NVIDIA sample applications to validate setup and benchmark performance.",
      "The [Isaac ROS](https://developer.nvidia.com/isaac-ros) ecosystem further accelerates robotics development. It provides GPU-optimized ROS2 packages for perception, SLAM, and navigation. These prebuilt nodes can be combined with custom ROS2 code to build complex robotics stacks quickly.",
      "Practical project examples include on-device object detection and semantic segmentation, SLAM-based navigation for mobile robots, multimodal assistants (voice + vision), and privacy-preserving smart camera systems. Edge inference removes latency and privacy issues tied to cloud offloading.",
      "Developers should account for thermal and memory constraints. Adequate cooling prevents throttling under sustained workloads, and the 8GB memory cap requires careful optimization of large models. Techniques like quantization, pruning, and batching strategies are recommended for transformer and vision models.",
      "Effective use of Jetson Orin Nano requires Linux experience, Python/ROS2 development skills, and knowledge of edge AI deployment practices. Leveraging TensorRT for model optimization and the Isaac ROS ecosystem for robotics pipelines maximizes the hardware’s potential.",
      "**Essential Resources:**<br/>- [Jetson Orin Developer Guide](https://developer.nvidia.com/embedded/jetson-orin)<br/>- [NVIDIA SDK Manager](https://developer.nvidia.com/nvidia-sdk-manager)<br/>- [Isaac ROS](https://developer.nvidia.com/isaac-ros)<br/>- [TensorRT Docs](https://docs.nvidia.com/deeplearning/tensorrt/)"
    ],
    "related": [
      "oak-d-pro",
      "amd-kria-kv260",
      "raspberry-pi-5"
    ]
  },
  {
    "id": "lenovo-legion-t5",
    "name": "Lenovo Legion Tower 5 Gen 10",
    "description": "A balanced workstation built with AMD Ryzen 7 7700X and NVIDIA RTX 5070 (12GB), 32GB DDR5-5600, and 1TB PCIe 4.0 NVMe. Suitable for gaming, AI inference, 3D content creation, XR prototyping, and multitasking workflows like rendering, compiling, and streaming.",
    "cloudinaryPublicId": "lenovo-legion-t5_ilvqhw",
    "categories": [
      "Workstation"
    ],
    "status": "available",
    "availableSince": "2025-09-10",
    "details": [
      "The Lenovo Legion Tower 5 Gen 10 bridges gaming-grade performance with professional creative and AI development needs. Powered by an AMD Ryzen 7 7700X CPU and an NVIDIA RTX 5070 GPU with 12GB VRAM, it combines 32GB of DDR5-5600 memory and a 1TB PCIe 4.0 NVMe SSD on the AMD B650 platform. This makes it a strong performer for 1440p/4K gaming, 3D creation, and GPU-accelerated machine learning tasks.",
      "The system is well-suited for 3D modeling, rendering, and video editing workflows, with GPU acceleration enabling smooth handling of large scenes. It also supports AI inference and small-to-medium training jobs, making it useful for ML experimentation and development. With strong multitasking performance, it can handle hybrid creative/technical workloads like XR demos, compiling, and streaming simultaneously.",
      "To start development, install the [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) and configure NVIDIA drivers for GPU workloads. For creative pipelines, set up [Blender](https://www.blender.org/) for 3D content creation and engines like [Unreal Engine](https://www.unrealengine.com/) or [Unity](https://unity.com/) for interactive development. For AI work, configure ML frameworks via [Anaconda](https://www.anaconda.com/) or Docker to maintain clean and isolated environments.",
      "The workflow benefits from containerization and environment management, as different projects may require unique versions of CUDA, Python, or libraries. This ensures reproducibility and avoids conflicts across domains (AI, graphics, XR).",
      "Potential applications include independent game development, XR/VR prototypes, Stable Diffusion image generation, small LLM experiments, and CAD/architectural visualization. Its GPU/CPU balance enables fast iteration cycles in both creative and research-oriented contexts.",
      "Limitations include the 12GB VRAM ceiling, which can constrain very large AI models or extremely complex 3D projects. Thermal design is tuned for consumer workloads—continuous 100% utilization over extended periods may cause throttling, so planning for adequate cooling and monitoring is recommended.",
      "Effective use requires familiarity with CUDA concepts, GPU driver management, and content creation or ML frameworks. Combining creative and technical skills will maximize productivity with this workstation.",
      "**Key Development Tools:**<br/>- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)<br/>- [Blender](https://www.blender.org/)<br/>- [Unreal Engine](https://www.unrealengine.com/) and [Unity](https://unity.com/)<br/>- [Anaconda](https://www.anaconda.com/)"
    ],
    "related": [
      "varjo-xr3",
      "shelby-computer"
    ]
  },
  {
    "id": "amd-kria-kr260",
    "name": "AMD Kria KR260 Robotics",
    "description": "FPGA-based robotics and industrial platform with ROS 2 acceleration, industrial interfaces, and low-latency processing. Built for developers building adaptive robotics and embedded AI solutions.",
    "cloudinaryPublicId": "amd-robotics_x3a9q3",
    "categories": [
      "Robotics",
      "FPGA"
    ],
    "status": "available",
    "availableSince": "2025-09-01",
    "details": [
      "The [AMD Kria KR260 Robotics Starter Kit](https://www.amd.com/en/products/system-on-modules/kria/k26/kr260-robotics-starter-kit.html) is a robotics-focused platform built on AMD/Xilinx adaptive SoC technology. It integrates FPGA fabric, a quad-core Arm Cortex-A53 processor, and industrial-grade interfaces for robotics, automation, and embedded AI applications.",
      "It delivers deterministic, low-latency control loops with hardware-accelerated ROS 2 nodes, enabling microsecond-level response times not achievable with CPU-only solutions. Developers can offload compute-heavy tasks (e.g., perception, sensor fusion, control algorithms) to the FPGA fabric while keeping ROS 2 software components running on Linux.",
      "Development starts with the [Kria Applications Documentation](https://xilinx.github.io/kria-apps-docs/), which includes prebuilt acceleration examples. Install the [Vitis](https://www.amd.com/en/products/software/adaptive-socs-and-fpgas/vitis.html) and [Vivado](https://www.amd.com/en/products/software/adaptive-socs-and-fpgas/vivado.html) toolchains for FPGA development and system integration. Provided reference pipelines let developers quickly test vision and control workloads before moving to custom hardware acceleration.",
      "A typical workflow involves deploying FPGA-accelerated pipelines alongside ROS 2 nodes for hybrid hardware/software systems. The modularity allows robotics developers to accelerate bottlenecks like image processing or real-time control while keeping higher-level autonomy in software.",
      "Example applications include factory automation, robotic arm control, high-speed inspection systems, and autonomous mobile robots with deterministic SLAM or vision pipelines. The platform is particularly suited for environments requiring reliable timing and industrial-grade interfaces.",
      "Challenges include the steeper learning curve of FPGA development, the need for specialized toolchains (Vitis/Vivado), and resource constraints compared to large FPGAs. Effective use requires familiarity with both embedded Linux and FPGA workflows, as debugging and optimization span hardware and software domains.",
      "Success with KR260 requires skills in ROS 2, embedded Linux, and FPGA design (HDL/HLS). Knowledge of real-time systems and industrial communication protocols is also valuable for fully leveraging the platform’s robotics and automation capabilities.",
      "**Essential Development Resources:**<br/>- [Kria Applications Docs](https://xilinx.github.io/kria-apps-docs/)<br/>- [ROS 2 Documentation](https://docs.ros.org/en/rolling/)<br/>- [Vitis Development Platform](https://www.amd.com/en/products/software/adaptive-socs-and-fpgas/vitis.html)<br/>- [Vivado Design Suite](https://www.amd.com/en/products/software/adaptive-socs-and-fpgas/vivado.html)"
    ],
    "related": [
      "nexys-a7-fpga",
      "amd-kria-kv260"
    ]
  },
  {
    "id": "amd-kria-kv260",
    "name": "AMD Kria KV260 Vision",
    "description": "FPGA-based vision AI platform designed for edge video analytics. Combines ARM processing with FPGA acceleration for real-time computer vision in smart city, industrial, and security applications.",
    "cloudinaryPublicId": "amd-vision_d4flw0",
    "categories": [
      "Computer Vision",
      "AI",
      "FPGA"
    ],
    "status": "available",
    "availableSince": "2025-09-01",
    "details": [
      "The [AMD Kria KV260 Vision AI Starter Kit](https://www.amd.com/en/products/system-on-modules/kria/k26/kv260-vision-starter-kit.html) is optimized for advanced computer vision applications. It integrates a quad-core Arm Cortex-A53 processor with FPGA fabric to accelerate real-time video analytics, deep neural network inference, and edge vision pipelines.",
      "The platform excels in smart city infrastructure, retail analytics, industrial inspection, and security systems. By running inference locally on the FPGA, it provides low-latency responses while maintaining privacy through on-premises data processing.",
      "Development begins with [Vitis](https://www.xilinx.com/products/design-tools/vitis.html) and [Vivado](https://www.xilinx.com/products/design-tools/vivado.html) for FPGA workflows. The [Vitis AI](https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html) stack provides pre-optimized DNN models and reference applications for vision use cases. Developers can start with these pipelines and extend them by customizing FPGA kernels and integrating new algorithms.",
      "Applications can be deployed as ROS 2 nodes for robotics systems or as [GStreamer](https://gstreamer.freedesktop.org/) pipelines for traditional multimedia and vision workflows. This flexibility allows the KV260 to fit both robotics and standalone edge vision deployments.",
      "Example projects include traffic monitoring with object detection and incident response, industrial inspection systems for defect detection, retail analytics for customer behavior, and privacy-preserving healthcare or security monitoring systems.",
      "While powerful for deterministic, real-time workloads, FPGA development requires specialized skills. The learning curve of HLS/RTL and FPGA workflows is higher than GPU-centric approaches, and resource limitations compared to larger devices should be considered when planning projects.",
      "Successful development requires skills in computer vision (OpenCV, deep learning models), FPGA acceleration workflows (HLS, Vitis AI), and real-time systems. Combining domain knowledge across these areas enables efficient custom vision pipelines at the edge.",
      "**Key Development Resources:**<br/>- [Vitis AI](https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html)<br/>- [Kria Documentation](https://xilinx.github.io/kria-apps-docs/)<br/>- [ROS 2](https://docs.ros.org/en/rolling/)<br/>- [GStreamer](https://gstreamer.freedesktop.org/)"
    ],
    "related": [
      "oak-d-pro",
      "jetson-orin-nano",
      "amd-kria-kr260"
    ]
  },
  {
    "id": "oak-d-pro",
    "name": "OAK-D Pro",
    "description": "Stereo depth AI camera with onboard neural inference, integrated IR illumination, and reliable 3D perception across varied lighting conditions. Designed for robotics, AR/VR, and computer vision applications.",
    "cloudinaryPublicId": "camera_g2vijm",
    "categories": [
      "Computer Vision",
      "Robotics"
    ],
    "status": "available",
    "availableSince": "2025-09-01",
    "details": [
      "The [OAK-D Pro](https://docs.luxonis.com/projects/hardware/en/latest/pages/DM9095.html) is an advanced computer vision camera that integrates stereo depth sensing with onboard AI inference. It includes IR illumination for consistent depth performance in low-light environments, making it useful for robotics, SLAM, and interactive applications across indoor and outdoor contexts.",
      "It produces stereo depth maps and point clouds while running neural networks for object detection, classification, and tracking directly on-device. This reduces the need for external compute, enabling efficient edge AI deployments in robotics, AR/VR, and embedded systems.",
      "Development starts with the [DepthAI SDK](https://docs.luxonis.com/software-v3/depthai/), which provides Python and C++ APIs. Example pipelines include stereo depth, object detection, and gesture recognition. These serve as practical starting points before customizing applications for specific robotics or vision workflows.",
      "Calibration and tuning are essential. Adjust exposure, IR illumination, and stereo depth parameters to optimize performance for your environment. Processed data can be streamed to Jetson modules, PCs, or other hosts, combining onboard inference with external compute when needed.",
      "Applications include autonomous robotics navigation, SLAM, people-tracking systems, gesture-based HMI, and mixed reality scene understanding. The OAK-D Pro's depth accuracy and integrated inference make it a strong fit for perception-driven systems that need local processing.",
      "Limitations include reduced depth accuracy under strong sunlight (which interferes with IR illumination) and compute limits that restrict very large or complex neural networks. Applications requiring heavy models may need hybrid processing strategies (part onboard, part external).",
      "Effective use requires Python or C++ skills, experience with OpenCV, and knowledge of camera calibration and 3D geometry. Understanding trade-offs between onboard and external inference helps optimize system performance.",
      "**Essential Development Resources:**<br/>- [DepthAI Docs](https://docs.luxonis.com/)<br/>- [DepthAI Model Zoo](https://github.com/luxonis/depthai-experiments)<br/>- [OpenCV Docs](https://docs.opencv.org/)<br/>- [DepthAI Community Forum](https://discuss.luxonis.com/)"
    ],
    "related": [
      "jetson-orin-nano",
      "varjo-xr3",
      "amd-kria-kv260"
    ]
  },
  {
    "id": "raspberry-pi-5",
    "name": "Raspberry Pi 5",
    "description": "Compact, high-performance single-board computers designed for education, prototyping, and production-ready embedded systems. Raspberry Pi boards offer powerful processing, rich connectivity, and a mature ecosystem suitable for robotics, IoT gateways, media systems, and edge computing.",
    "images": [
      "https://mm.digikey.com/Volume0/opasdata/d220001/derivates/6/002/156/728/MFG_PI_5_HERO_web%28640x640%29.jpg"
    ],
    "imageCaptions": [
      "Raspberry Pi 5 (8GB)"
    ],
    "categories": [
      "Embedded Systems",
      "IoT",
      "Prototyping"
    ],
    "status": "available",
    "availableSince": "2026-01-15",
    "details": [
      "The [Raspberry Pi 5](https://www.raspberrypi.com/products/raspberry-pi-5/) features a quad-core Arm Cortex-A76 processor (2.4GHz), 8GB RAM, dual 4Kp60 HDMI outputs, PCIe 2.0, Wi-Fi 802.11ac, Bluetooth 5.0/BLE, Gigabit Ethernet, and USB 3.0.",
      "**Key Features:** PCIe 2.0 interface supports NVMe SSDs, AI accelerators (Coral TPU), and network cards. Power button and real-time clock included. Use official 27W USB-C adapter to avoid power issues.",
      "**Use Cases:** IoT gateways, edge computing with lightweight ML, 4K media centers, home automation, robotics with vision, and educational projects. Mature ecosystem with extensive community support.",
      "**Getting Started:** Flash [Raspberry Pi OS](https://www.raspberrypi.com/software/) to microSD using [Raspberry Pi Imager](https://www.raspberrypi.com/software/), connect peripherals, boot. Enable SSH/VNC for remote access. Use Python with GPIO libraries or [Docker](https://docs.docker.com/) for containerized apps.",
      "**Prerequisites:** Basic Linux CLI skills, Python for GPIO/sensors. Consider heatsink or active cooling for sustained loads. Requires basic understanding of power management and thermal design.",
      "**Compatible Accessories:** Raspberry Pi 5 works with our [starter kits, sensors, Camera Module 3, and displays](/hardware/raspberry-pi-accessories). The GPIO pins are compatible with sensors from all SunFounder kits, and the CSI interface supports the official camera module.",
      "**Available:** Raspberry Pi 5 (8GB RAM) - [Datasheet](https://www.raspberrypi.com/documentation/)",
      "**Essential Resources:**<br/>- [Raspberry Pi Documentation](https://www.raspberrypi.com/documentation/)<br/>- [Raspberry Pi OS](https://www.raspberrypi.com/software/)<br/>- [GPIO Zero Library](https://gpiozero.readthedocs.io/)<br/>- [Raspberry Pi Forums](https://forums.raspberrypi.com/)"
    ],
    "related": [
      "jetson-orin-nano",
      "esp-dev-boards",
      "raspberry-pi-accessories"
    ]
  },
  {
    "id": "esp-dev-boards",
    "name": "ESP Development Boards",
    "description": "Wireless-first development platforms optimized for IoT and connected hardware. ESP boards combine Wi-Fi, Bluetooth, and low-power communication technologies, enabling rapid prototyping and scalable deployment of smart devices and sensors.",
    "images": [
      "https://mm.digikey.com/Volume0/opasdata/d220001/medias/images/6253/MFG_DFR1117.jpg",
      "https://mm.digikey.com/Volume0/opasdata/d220001/medias/images/4899/ESP32-C6-DEVKITC-1-N8.jpg",
      "https://mm.digikey.com/Volume0/opasdata/d220001/derivates/6/003/231/483/MFG_ESP32-S3-DevKitC-1-N32R16V_web%28640x640%29.jpg"
    ],
    "imageCaptions": [
      "DFRobot Beetle ESP32-C6 Mini",
      "ESP32-C6 DevKitC-1 (N8)",
      "ESP32-S3-WROOM-2-N32R16V"
    ],
    "categories": [
      "IoT",
      "Embedded Systems",
      "Prototyping"
    ],
    "status": "available",
    "availableSince": "2026-01-15",
    "details": [
      "ESP32 boards integrate Wi-Fi, Bluetooth LE, Wi-Fi 6, and Thread/Zigbee (802.15.4) for wireless IoT prototyping. Ideal for battery-powered sensors, smart home devices, wearables, and industrial monitoring.",
      "**Three Boards Available:** **DFRobot Beetle ESP32-C6** - ultra-compact with Wi-Fi 6, BLE, 802.15.4 for wearables. **ESP32-C6 DevKitC-1 (N8)** - general-purpose, Matter-compatible. **ESP32-S3-WROOM-2-N32R16V** - 32MB flash, 16MB PSRAM, native USB for AI inference and camera apps.",
      "**Key Features:** ESP32-C6 adds Wi-Fi 6 and IEEE 802.15.4 for Matter smart home integration. ESP32-S3 focuses on AI/HMI with enhanced processing and memory. Low power modes enable years of battery life.",
      "**Use Cases:** Environmental sensors, BLE beacons, Wi-Fi actuators, battery-powered loggers, edge AI (TensorFlow Lite) for keyword spotting/gesture recognition. Rich peripherals: I2C, SPI, UART, ADC, PWM, I2S.",
      "**Getting Started:** Install [Arduino IDE](https://www.arduino.cc/en/software) or [PlatformIO](https://platformio.org/), add ESP32 board support, upload sketches. For production, use [ESP-IDF](https://docs.espressif.com/projects/esp-idf/) for power management and security. Cloud integrations: AWS IoT, Google Cloud IoT, Azure IoT Hub.",
      "**Prerequisites:** C/C++ programming, embedded systems (interrupts, timers, DMA), wireless protocols (Wi-Fi, BLE, MQTT), low-power design. FreeRTOS knowledge helpful for advanced projects.",
      "**Compatible Accessories:** ESP32 boards work with our [cross-platform sensor kits and displays](/hardware/raspberry-pi-accessories). The SunFounder Sensor Kit includes example code specifically for ESP32, and e-ink displays connect via SPI.",
      "**Available Boards:**<br/>- DFRobot Beetle ESP32-C6 Mini (Wi-Fi 6, BLE, 802.15.4) - [Datasheet](https://mm.digikey.com/Volume0/opasdata/d220001/medias/docus/5528/ESP32-C6-DEVKITC-1-N8.pdf)<br/>- ESP32-C6 DevKitC-1-N8 (Wi-Fi 6, BLE, 802.15.4) - [Datasheet](https://mm.digikey.com/Volume0/opasdata/d220001/medias/docus/5528/ESP32-C6-DEVKITC-1-N8.pdf)<br/>- ESP32-S3-WROOM-2-N32R16V (32MB Flash, 16MB PSRAM, USB) - [Datasheet](https://docs.espressif.com/projects/esp-dev-kits/en/latest/esp32s3/esp-dev-kits-en-master-esp32s3.pdf)",
      "**Essential Resources:**<br/>- [ESP-IDF Documentation](https://docs.espressif.com/projects/esp-idf/)<br/>- [Arduino Core for ESP32](https://github.com/espressif/arduino-esp32)<br/>- [PlatformIO](https://platformio.org/)<br/>- [Espressif Developer Portal](https://developer.espressif.com/)"
    ],
    "related": [
      "raspberry-pi-5",
      "raspberry-pi-accessories",
      "alifensemble"
    ]
  },
  {
    "id": "openbci-biosensing-bundle",
    "name": "OpenBCI Biosensing Starter Bundle",
    "description": "Complete starter bundle for electro-physiological data acquisition. Includes Ganglion board, electrodes, and headband kit for real-time EEG, ECG, and EMG recording with open-source software.",
    "images": [
      "https://shop.openbci.com/cdn/shop/files/GANG_39b78a60-781d-480d-bd10-ab15f0fdb972.png?v=1700591381&width=1962",
      "https://shop.openbci.com/cdn/shop/files/Headband-kit-full.webp?v=1708550401&width=1418"
    ],
    "imageCaptions": [
      "OpenBCI Ganglion Board",
      "OpenBCI EEG Headband Kit"
    ],
    "categories": [
      "Biosensing"
    ],
    "status": "available",
    "availableSince": "2026-01-28",
    "details": [
      "The [OpenBCI Biosensing Starter Bundle](https://shop.openbci.com/) is a complete system for recording EEG (brain), ECG (heart), and EMG (muscle) signals using open-source software and hardware.",
      "**Bundle Contents:**<br/>- Ganglion Board (4-channel, 32-bit biosensing board with Bluetooth LE)<br/>- Battery and USB charger<br/>- Ganglion USB Dongle (wireless connectivity)<br/>- EMG/ECG Gel Snap Electrodes<br/>- EMG/ECG Snap Cables<br/>- OpenBCI EEG Headband Kit (adjustable with dry electrode holders)",
      "**Technical Specs:** 4-channel acquisition, up to 1600 Hz sampling per channel, >1 GΩ input impedance, onboard microSD storage, wireless Bluetooth operation. Headband supports standard 10-20 electrode placement for frontal lobe monitoring.",
      "**Applications:** Brain-computer interfaces, neurofeedback, cognitive research, emotion detection, heart rate variability, muscle activation studies, rehabilitation monitoring, and HCI prototyping.",
      "**Software & Development:** Use the free [OpenBCI GUI](https://openbci.com/downloads) for real-time visualization. Stream data via LSL, OSC, or UDP to custom applications. Open-source libraries available for Python, Java, Node.js. Compatible with MATLAB, Python (MNE, SciPy), and EEGLab.",
      "**Expansion Options:** Add Gold Cup Electrodes with Ten20 Paste for research-grade wet EEG. Upgrade to Cyton (8-channel) or add multiple Ganglion boards for higher-density recordings.",
      "**Prerequisites:** Understanding of biosignal acquisition, electrode placement, signal processing. Python or similar for custom pipelines. Basic knowledge of experimental design for physiological research.",
      "**Essential Resources:**<br/>- [OpenBCI Documentation](https://docs.openbci.com/)<br/>- [OpenBCI GUI Software](https://openbci.com/downloads)<br/>- [OpenBCI GitHub](https://github.com/OpenBCI)<br/>- [OpenBCI Community Forum](https://openbci.com/forum/)"
    ],
    "related": [
      "emotibit-bundle",
      "doublepoint-dev-kit",
      "varjo-xr3"
    ]
  },
  {
    "id": "emotibit-bundle",
    "name": "EmotiBit Bundle",
    "description": "Wearable biometric sensor platform for capturing physiological and emotional data. Measures PPG, EDA, temperature, accelerometer, and more for research, health monitoring, and human-computer interaction applications.",
    "images": [
      "https://www.emotibit.com/wp-content/uploads/2024/07/squ_EmotiBit_Montgomery_01_754x_e6935b79-598d-4ff7-b7a0-68918811fdf4.webp",
      "https://www.emotibit.com/wp-content/uploads/2024/06/squ_01All-in-OneEmotiBitBundle01.png"
    ],
    "imageCaptions": [
      "EmotiBit Sensor Module",
      "EmotiBit All-in-One Bundle"
    ],
    "categories": [
      "Biosensing"
    ],
    "status": "available",
    "availableSince": "2026-01-28",
    "details": [
      "The [EmotiBit All-in-One Bundle](https://www.emotibit.com/) is a wearable multi-sensor platform for capturing physiological and emotional data in real-time. Designed for research, health monitoring, and affective computing applications.",
      "**Bundle Contents:**<br/>- EmotiBit Sensor Module (wearable biometric sensor)<br/>- Battery and charging cable<br/>- Wearable straps and mounting accessories<br/>- SD card for local data storage<br/>- EmotiBit software suite and visualization tools",
      "**Integrated Sensors:** PPG (heart rate, HRV), EDA (skin conductance, arousal), thermopile (skin temperature), 9-axis IMU (3-axis accelerometer, gyroscope, magnetometer). All sensors stream simultaneously at research-grade resolution.",
      "**Connectivity & Data:** WiFi and Bluetooth wireless streaming. Onboard SD card logging for untethered field studies. Battery-powered for hours of continuous operation. Real-time data transmission to custom applications.",
      "**Applications:** Affective computing, stress and emotion monitoring, HCI research, biofeedback systems, UX research, health and wellness tracking, sports performance, VR/AR physiological integration.",
      "**Software & Development:** EmotiBit Oscilloscope for real-time visualization. Stream data via OSC (Open Sound Control) to Python, Unity, and other environments. Compatible with Python data analysis pipelines and third-party visualization tools.",
      "**Prerequisites:** Understanding of biometric signals, signal processing basics, physiological measurement principles. Python or similar programming for custom analysis and integration.",
      "**Essential Resources:**<br/>- [EmotiBit Documentation](https://www.emotibit.com/)<br/>- [EmotiBit GitHub](https://github.com/EmotiBit)<br/>- [Getting Started Guide](https://www.emotibit.com/)<br/>- [EmotiBit Community](https://www.emotibit.com/)"
    ],
    "related": [
      "openbci-biosensing-bundle",
      "doublepoint-dev-kit",
      "varjo-xr3"
    ]
  },
  {
    "id": "raspberry-pi-accessories",
    "name": "Kits & Accessories for RPi, Arduino & ESP32",
    "description": "Starter kits, sensor kits, cameras, and displays designed to extend and enhance ESP, Arduino, and Raspberry Pi projects. These accessories support learning, experimentation, and rapid system expansion.",
    "images": [
      "https://mm.digikey.com/Volume0/opasdata/d220001/derivates/6/003/227/919/MFG_CN0434D_web%28640x640%29.jpg",
      "https://mm.digikey.com/Volume0/opasdata/d220001/medias/images/4412/MFG_CN0348D.jpg",
      "https://mm.digikey.com/Volume0/opasdata/d220001/derivates/6/003/227/919/MFG_CN0349D_web%28640x640%29.jpg",
      "https://mm.digikey.com/Volume0/opasdata/d220001/derivates/6/003/214/269/MFG_CN0450D_web%28640x640%29.jpg",
      "https://mm.digikey.com/Volume0/opasdata/d220001/medias/images/5014/MFG_SC0872.jpg",
      "https://mm.digikey.com/Volume0/opasdata/d220001/derivates/6/003/238/180/MFG_6373-00_web%28640x640%29.jpg"
    ],
    "imageCaptions": [
      "SunFounder Ultimate Starter Kit (CN0434D)",
      "SunFounder RPi Starter Kit #1 (CN0348D)",
      "SunFounder RPi Starter Kit #2 (CN0349D)",
      "SunFounder Sensor Kit (CN0450D)",
      "Raspberry Pi Camera Module 3",
      "Adafruit 2.13\" E-Ink Display"
    ],
    "categories": [
      "Prototyping"
    ],
    "status": "available",
    "availableSince": "2026-01-15",
    "details": [
      "A comprehensive collection of starter kits, sensors, cameras, and displays compatible with Raspberry Pi, Arduino, and ESP32 platforms. Designed for learning, prototyping, and building complete embedded systems.",
      "**Kit Collection:**<br/>- SunFounder Ultimate Starter Kit (CN0434D) - comprehensive sensor collection and project tutorials<br/>- SunFounder RPi Starter Kit #1 (CN0348D) - GPIO basics and essential components<br/>- SunFounder RPi Starter Kit #2 (CN0349D) - intermediate GPIO projects<br/>- SunFounder Sensor Kit (CN0450D) - cross-platform sensors for Arduino, ESP32, and Raspberry Pi",
      "**Camera & Vision:** Raspberry Pi Camera Module 3 features 12MP resolution, autofocus, and HDR support. Ideal for computer vision and AI detection projects. Use with the [picamera2](https://github.com/raspberrypi/picamera2) library for Python integration.",
      "**Displays:** Adafruit 2.13\" E-Ink Display (6373) offers ultra-low power consumption with 4-color support. E-Ink Breakout Friend (4224) provides buffering and logic shifting for e-ink display integration.",
      "**Applications:** GPIO programming, sensor integration, computer vision, IoT prototyping, robotics projects, and embedded system development. All kits include wiring guides and example code.",
      "**Prerequisites:** Basic electronics knowledge, Python or C++ programming, understanding of GPIO interfaces and communication protocols (I2C, SPI, UART).",
      "**Available Items:**<br/>- SunFounder Ultimate Starter Kit (CN0434D) - [Datasheet](https://mm.digikey.com/Volume0/opasdata/d220001/medias/docus/5561/Raphael%20Kit.pdf)<br/>- SunFounder RPi Starter Kit #1 (CN0348D)<br/>- SunFounder RPi Starter Kit #2 (CN0349D) - [Datasheet](https://mm.digikey.com/Volume0/opasdata/d220001/medias/docus/5561/DaVinci%20Kit.pdf)<br/>- SunFounder Sensor Kit (CN0450D) - [Datasheet](https://mm.digikey.com/Volume0/opasdata/d220001/medias/docus/6448/CN0450D%20datasheet.pdf)<br/>- Raspberry Pi Camera Module 3 (SC1223) - [Docs](https://www.raspberrypi.com/documentation/accessories/camera.html)<br/>- Adafruit 2.13\" E-Ink Display (6373) - [Datasheet](https://cdn-shop.adafruit.com/product-files/6373/C22164-001_datasheet__ZJY122250-0213AJH-E5______.pdf)<br/>- Adafruit E-Ink Breakout Friend (4224)",
      "**Essential Resources:**<br/>- [Raspberry Pi GPIO Documentation](https://www.raspberrypi.com/documentation/computers/raspberry-pi.html)<br/>- [Arduino Documentation](https://docs.arduino.cc/)<br/>- [Adafruit Learning System](https://learn.adafruit.com/)<br/>- [SunFounder Learning Platform](https://docs.sunfounder.com/)"
    ],
    "related": [
      "raspberry-pi-5",
      "esp-dev-boards",
      "jetson-orin-nano"
    ]
  },
  {
    "id": "seetrue-eye-tracker",
    "name": "Mobile eye tracker ST ONE",
    "description": "Research-grade wearable eye-tracking glasses delivering robust, slip-tolerant, and recalibration-free gaze tracking for mobile, real-world, and XR research scenarios.",
    "cloudinaryPublicId": "st-one-eye-tracker",
    "categories": [
      "Biosensing",
      "XR/VR"
    ],
    "status": "coming-soon",
    "availableSince": "2026-02-06",
    "details": [
      "ST ONE is a wearable, glasses-form-factor eye-tracking system designed for collecting high-quality eye-movement data in real-world and mobile conditions where head motion, device slippage, and interruptions are expected. Unlike many eye trackers optimized for static lab setups, ST ONE is built for naturalistic behavior, allowing rapid head movement, re-wearing between sessions, and use with corrective eyewear.",
      "The system relies on infrared cameras and a binocular, truly 3D eye model to measure real eye movements directly, rather than estimating gaze via end-to-end deep learning. This approach improves robustness across diverse users and environments and supports consistent accuracy without frequent recalibration. A key feature is persistent calibration: once calibrated, users can remove and re-don the device without repeating calibration procedures.",
      "ST ONE records eye-tracking data synchronized with a forward-facing scene camera, enabling contextual gaze analysis in real-world tasks. Captured signals include eye position and orientation (x, y, z), gaze point coordinates, pupil size (in millimeters), blink events, fixation events, and validity codes. Data can be streamed live or recorded for offline analysis.",
      "For development and integration, recorded data can be exported in standard formats such as CSV and gaze video, allowing straightforward use in Python, MATLAB, or R analysis pipelines. Real-time streaming enables multimodal experiments combining eye tracking with EEG, physiological signals, motion capture, or XR systems.",
      "Typical applications include cognitive and behavioral science, usability and UX research, XR interaction studies, training and performance analysis, and human–computer interaction experiments that require unrestricted head and body movement.",
      "**Development & Research Advice:** Validate calibration stability and data quality under your expected motion conditions before running full experiments. When combining ST ONE with XR headsets or motion capture, use a shared timestamping or synchronization strategy. For real-time applications, prototype first with recorded data to tune latency and filtering before deploying live pipelines."
    ],
    "related": [
      "varjo-xr3",
      "mobile-mocap-pro-xyn",
      "openbci-biosensing-bundle"
    ]
  },
  {
    "id": "mobile-mocap-pro-xyn",
    "name": "Sony mocopi Pro Kit",
    "description": "Portable professional motion capture system combining Sony mocopi wearable sensors with XYN Motion Studio software for fast, full-body motion capture in XR, animation, and research workflows.",
    "cloudinaryPublicId": "sony-mocopi-Pro-Kit",
    "categories": [
      "Motion Capture",
      "XR/VR",
      "Computer Vision"
    ],
    "isNew": true,
    "status": "available",
    "availableSince": "2026-02-06",
    "images": [
      "https://d1ncau8tqf99kp.cloudfront.net/converted/114935_original_local_1200x1050_v3_converted.webp",
      "https://d1ncau8tqf99kp.cloudfront.net/converted/124226_original_local_1200x1050_v3_converted.webp",
      "https://d1ncau8tqf99kp.cloudfront.net/converted/124039_original_local_1200x1050_v3_converted.webp",
      "https://d1ncau8tqf99kp.cloudfront.net/converted/124041_original_local_1200x1050_v3_converted.webp"
    ],
    "imageCaptions": [
      "Sony mocopi Pro Kit",
      "Sony mocopi Pro Kit – full wearable set",
      "Wearable sensor placement on the body",
      "Demonstration of the Sony mocopi Pro Kit software in action"
    ],
    "details": [
      "The Sony mocopi Pro Kit is a lightweight, wearable motion capture system designed for rapid full-body motion capture without the need for external cameras or fixed studio installations. The system uses multiple compact IMU-based sensors worn on the body, enabling fast setup and portable capture in labs, studios, classrooms, or field environments.",
      "Motion data is processed using XYN Motion Studio, which provides tools for pose reconstruction, motion cleanup, retargeting, and export. The software supports common animation and real-time engines, allowing captured motion to be applied directly to digital characters, avatars, or simulation models with minimal post-processing.",
      "Captured motion can be exported in standard formats such as FBX and BVH, enabling integration with Unity, Unreal Engine, Blender, and custom biomechanics or robotics pipelines. This makes the system suitable for both real-time XR embodiment and offline analysis workflows.",
      "Typical use cases include character animation, XR embodiment and avatar control, virtual production prototyping, biomechanics and movement studies, sports and performance analysis, and human–computer interaction research.",
      "Compared to camera-based optical systems, mocopi prioritizes mobility and fast iteration. While IMU-based capture may exhibit drift or reduced absolute accuracy over long sessions, it enables motion capture in environments where optical systems are impractical or impossible.",
      "**Development & Research Advice:** Validate sensor placement consistency before long recordings and test retargeting with simple avatar rigs before moving to complex characters. When combining mocopi with eye tracking or biosensing, synchronize data streams at the software level using timestamps or a shared clock rather than relying on post-hoc alignment."
    ],
    "related": [
      "varjo-xr3",
      "doublepoint-dev-kit",
      "oak-d-pro"
    ]
  },
  { "id": "bambu-lab-p1s-ams2-pro", "name": "Bambu Lab P1S + AMS 2 Pro Combo", "description": "Enclosed high-speed CoreXY 3D printer with AMS 2 Pro for multi-material printing and rapid prototyping of functional parts, enclosures, and mechanical components.", "cloudinaryPublicId": "bambu-p1s-ams2-pro", "categories": ["Digital Fabrication", "Prototyping", "Robotics"], "status": "coming-soon", "availableSince": "2026-02-11", "images": [ "https://3d.nice-cdn.com/upload/image/product/large/default/bambu-lab-p1s-combo-826635-fi.png" ], "imageCaptions": [ "Bambu Lab P1S (P1 series image)" ], "details": [ "Bambu Lab P1S is an enclosed CoreXY 3D printer optimized for fast, repeatable printing of prototypes and functional parts. The enclosure improves reliability for temperature-sensitive filaments and reduces warping for larger parts.", "The AMS 2 Pro enables automatic filament switching for multi-material and multi-color prints, supports faster iteration for mechanical assemblies, and simplifies workflows like printing soluble supports or multi-part prototypes in one job.", "Typical use cases include robotics brackets, sensor mounts, custom enclosures, quick mechanical iterations, and fixtures/jigs for lab setups.", "**Development & Lab Advice:** For functional parts, validate tolerances with quick calibration prints (holes, press-fits) before long jobs. Prefer PETG/ABS/ASA for durable prototypes; plan ventilation when printing higher-temperature materials." ], "related": ["raspberry-pi-5", "esp-dev-boards", "amd-kria-kr260"] },  
  {
    "id": "ultracortex-mark-iv-cyton",
    "name": "Ultracortex Mark IV EEG Headset + Cyton Biosensing Board",
    "description": "Open-source EEG setup combining the Ultracortex Mark IV headset with the Cyton 8-channel biosensing board for EEG/EMG/ECG acquisition, BCI prototyping, and neurotech research.",
    "cloudinaryPublicId": "ultracortex-mark-iv-cyton",
    "categories": [
      "Biosensing",
      "BCI",
      "Neurotechnology"
    ],
    "status": "coming-soon",
    "availableSince": "2026-02-11",
    "images": [
      "https://shop.openbci.com/cdn/shop/products/DSC02861.jpg?v=1703102384&width=1080",
      "https://shop.openbci.com/cdn/shop/files/CYTON.png?v=1700590601&width=1260"
    ],
    "imageCaptions": [
      "ULTRACORTEX MARK IV EEG HEADSET",
      "CYTON BIOSENSING BOARD (8-CHANNELS)"
    ],
    "details": [
      "Ultracortex Mark IV is a modular EEG headset architecture designed for flexible electrode placement and repeatable positioning, suitable for student projects as well as research prototypes.",
      "Cyton is an 8-channel biosensing board commonly used for EEG, EMG, and ECG experiments. It supports real-time streaming workflows and integration with OpenBCI tooling and open-source analysis stacks.",
      "Common applications include neurofeedback prototypes, EEG-based interaction experiments, BCI pipelines (filtering → features → classification), and multimodal research when synchronized with XR or motion capture.",
      "**Development & Research Advice:** Prioritize electrode contact quality (impedance checks) and motion artifact mitigation. If combining with XR/eye tracking, implement shared timestamping early (e.g., LSL) to avoid painful post-hoc alignment."
    ],
    "related": [
      "openbci-biosensing-bundle",
      "openbci-exg-experiment-bundle",
      "seetrue-eye-tracker"
    ]
  },
  {
    "id": "openbci-exg-experiment-bundle",
    "name": "OpenBCI EXG Experiment Bundle",
    "description": "Hands-on experiment bundle for recording and learning from electrophysiology signals (EEG/EMG/ECG), designed for education, workshops, and rapid biosensing prototyping.",
    "cloudinaryPublicId": "openbci-exg-experiment-bundle",
    "categories": [
      "Biosensing",
      "Education",
      "Prototyping"
    ],
    "status": "coming-soon",
    "availableSince": "2026-02-11",
    "images": [
      "https://shop.openbci.com/cdn/shop/files/ExperimentBundleProductImage.png?v=1762555145&width=1080",
      "https://shop.openbci.com/cdn/shop/files/ganglion-experiment-bundle-photo-WHITE.png?v=1762554493&width=1080"
    ],
    "imageCaptions": [
      "OpenBCI EXG Experiment Bundle (overview)",
      "Experiment bundle (Ganglion-based photo)"
    ],
    "details": [
      "The EXG Experiment Bundle is assembled for quick-start electrophysiology experiments and workshops, reducing setup friction for first-time users and student teams.",
      "It supports introductory-to-intermediate experiments across EEG/EMG/ECG, and is well-suited for building reproducible lab demos, hackathon tracks, and teaching signal processing fundamentals.",
      "Data can be streamed to OpenBCI tools and exported for analysis in Python/MATLAB pipelines (filtering, artifact removal, feature extraction, and classification).",
      "**Development & Research Advice:** Start with controlled conditions (minimal movement) for baseline recordings, then deliberately introduce motion/interaction to characterize artifacts. For ML demos, collect more labeled data than you think you need—especially across multiple users."
    ],
    "related": [
      "openbci-biosensing-bundle",
      "ultracortex-mark-iv-cyton",
      "emotibit-bundle"
    ]
  }
]